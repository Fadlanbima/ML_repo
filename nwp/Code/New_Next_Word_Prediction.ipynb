{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrwNKmzAI9Wb"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W4/ungraded_labs/C3_W4_Lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Next Word Prediction**"
      ],
      "metadata": {
        "id": "ni9PR8pQ5mkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install tensorflowjs"
      ],
      "metadata": {
        "id": "RHpkVuuQayax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs"
      ],
      "metadata": {
        "id": "ZFpMbNR6apZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries\n",
        "\n",
        "In this section, we need to import all the required libraries. Here all those libraries."
      ],
      "metadata": {
        "id": "6anwEhn0tr-C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOwsuGQQY9OL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Word Vocabulary\n",
        " To get all the word vocabulary from destination list, here we use the data that we've gathered and saved on Google Sheet and simply inserted it in destination variable. "
      ],
      "metadata": {
        "id": "3TLUR49uwM89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/Fadlanbima/ML_repo/master/nwp/Destination.csv')"
      ],
      "metadata": {
        "id": "-fme1v9eQRYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "1o6Q1zsuSOmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df['Destinasi']"
      ],
      "metadata": {
        "id": "qntqBI3atmVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate each word index dictionary and compute the total words"
      ],
      "metadata": {
        "id": "F4PkKVW_ysWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRnDnCW-Z7qv"
      },
      "outputs": [],
      "source": [
        "# Initialize Tokenizer class\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Generate the word index dictionary\n",
        "tokenizer.fit_on_texts(df)\n",
        "\n",
        "# Define the total words. Add 1 for the index `0` as the padding token.\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f'word index dictionary: {tokenizer.word_index}')\n",
        "print(f'total words: {total_words}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the Dataset\n",
        "\n",
        "Here we will generate the training sequences which the result would be inputs as padded sequences and the labels will be labelled as one-hot encoded arrays."
      ],
      "metadata": {
        "id": "Iobr8ocTzJKW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soPGVheskaQP"
      },
      "outputs": [],
      "source": [
        "# Initialize the sequences list\n",
        "input_sequences = []\n",
        "\n",
        "# Loop over every line\n",
        "for line in df:\n",
        "\n",
        "    # Tokenize the current line\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "    # Loop over the line several times to generate the subphrases\n",
        "    for i in range(1, len(token_list)):\n",
        "        \n",
        "        # Generate the subphrase\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "\n",
        "        # Append the subphrase to the sequences list\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Get the length of the longest line\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "# Pad all sequences\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding='pre'))\n",
        "\n",
        "# Create inputs and label by splitting the last token in the subphrases\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "# Convert the label into one-hot arrays\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes = total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Model"
      ],
      "metadata": {
        "id": "5mp8jy5Z3DHX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9vH8Y59ajYL"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "          Embedding(total_words, 64, input_length = max_sequence_len-1),\n",
        "          LSTM(100, return_sequences=True),\n",
        "          LSTM(100),\n",
        "          Dense(50, activation='relu'),\n",
        "          Dense(total_words, activation = 'softmax')])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.002),\n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Show the Model Summary"
      ],
      "metadata": {
        "id": "fnIdeUf240DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "KlZ4GEIF4wjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the Model"
      ],
      "metadata": {
        "id": "4lmOWMpsHOhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model, to_file='plot.png', show_layer_names=True)"
      ],
      "metadata": {
        "id": "GpIm9J98HWhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ],
      "metadata": {
        "id": "hIELs86q4Heo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF2k2nD9vgsD"
      },
      "outputs": [],
      "source": [
        "history = model.fit(xs, ys, epochs = 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visual the Result"
      ],
      "metadata": {
        "id": "9mBQMRdD4MV6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YXGelKThoTT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot utility\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "# Visualize the accuracy\n",
        "plot_graphs(history, 'accuracy')\n",
        "plot_graphs(history, 'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Prediction"
      ],
      "metadata": {
        "id": "w4RkdvkN5NBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_array = np.array(list(tokenizer.word_index.keys()))\n",
        "\n",
        "def make_prediction(text, n_words):\n",
        "    for i in range(n_words):\n",
        "        text_tokenize = tokenizer.texts_to_sequences([text])\n",
        "        text_padded = tf.keras.preprocessing.sequence.pad_sequences(text_tokenize, maxlen = max_sequence_len-1)\n",
        "        prediction = np.squeeze(np.argmax(model.predict(text_padded), axis = -1))\n",
        "        prediction = str(vocab_array[prediction - 1])\n",
        "        text += \" \" + prediction.capitalize()\n",
        "    return text"
      ],
      "metadata": {
        "id": "IxgT45Mf5SHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text_new = input(\"Enter your destination: \")"
      ],
      "metadata": {
        "id": "H1m_6J09HkWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_prediction(\"Taman Mini\", 2)"
      ],
      "metadata": {
        "id": "SCnjlsUUQTkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Model"
      ],
      "metadata": {
        "id": "1Hb9rqP94qK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_path = \"nwp_model.h5\"\n",
        "\n",
        "model.save(saved_model_path)"
      ],
      "metadata": {
        "id": "Sn65MIZ1qHLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To JSON"
      ],
      "metadata": {
        "id": "VwPamRDtN5Yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorflowjs_converter --input_format=keras {saved_model_path} ./"
      ],
      "metadata": {
        "id": "gZYk890gN7cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To pickle"
      ],
      "metadata": {
        "id": "r75UJ-ZoQGVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "Pr6Wlp-7R0h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name='nwp_model.pkl'\n",
        "pickle.dump(model, open(file_name, 'wb')) "
      ],
      "metadata": {
        "id": "nciV3PZvQRc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If wanna load the model"
      ],
      "metadata": {
        "id": "x73zit0PQTxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = pickle.load(open(file_name, 'rb'))\n",
        "result = loaded_model(xs, ys)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "UihmOdvzQXBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Pb"
      ],
      "metadata": {
        "id": "-sGrIVTKj7az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "\n",
        "tmpdir = tempfile.mkdtemp()"
      ],
      "metadata": {
        "id": "n2K5pzjmj-i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet_save_path = os.path.join(tmpdir, \"SavedModel\")\n",
        "tf.saved_model.save(model, mobilenet_save_path)"
      ],
      "metadata": {
        "id": "10rUGypgkAJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls {mobilenet_save_path}"
      ],
      "metadata": {
        "id": "2ECQ47PCkBsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To TF Lite"
      ],
      "metadata": {
        "id": "QyBYI4sI_dXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir='/tmp/tmpalaqkqms/SavedModel', signature_keys=['serving_default'])\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "tflite_model = converter.convert()"
      ],
      "metadata": {
        "id": "wxsVx08tIQze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show model size in KBs.\n",
        "float_model_size = len(tflite_model) / 1024\n",
        "print('Float model size = %dKBs.' % float_model_size)"
      ],
      "metadata": {
        "id": "RM48ZjB6MUTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show model size in KBs.\n",
        "quantized_model_size = len(tflite_model) / 1024\n",
        "print('Quantized model size = %dKBs,' % quantized_model_size)\n",
        "print('which is about %d%% of the float model size.'\\\n",
        "      % (quantized_model_size * 100 / float_model_size))"
      ],
      "metadata": {
        "id": "M2pTnoTbMJ6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the quantized model to file to the Downloads directory\n",
        "f = open('nwp_model.tflite', \"wb\")\n",
        "f.write(tflite_model)\n",
        "f.close()\n",
        "\n",
        "# Download the digit classification model\n",
        "from google.colab import files\n",
        "files.download('nwp_model.tflite')\n",
        "\n",
        "print('`nwp_model.tflite` has been downloaded')"
      ],
      "metadata": {
        "id": "TLQCnbqxMex7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "New_Next_Word_Prediction",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}